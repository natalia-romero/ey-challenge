{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59b73dc2-18ec-4481-8a05-c8b9b93ebde9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Phase 1: Tropical Cyclone Damage Assesment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4f124-29c9-4ced-bed8-4dbabc5443c6",
   "metadata": {},
   "source": [
    "## Challenge Phase 1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621a62f-a8b6-4683-aec2-819314e74d6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align=\"justify\">Welcome to the EY Open Science Data Challenge 2024 ! This challenge consists of two phases – Phase 1 and Phase 2. In Phase 1 of the challenge, participants will be provided with high-resolution pre and post event satellite images from Maxar GeoEye-1 of an area impacted by tropical cyclone, as well as moderate-resolution data from the European Sentinel-2 (optical) and Landsat (Optical) satellites. Participants will have to develop a machine learning model to identify and detect “damaged” and “un-damaged” coastal infrastructure (residential and commercial buildings), which get impacted by natural calamities like hurricanes, cyclones, etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90f37e-f956-4c80-8624-f028de914330",
   "metadata": {
    "tags": []
   },
   "source": [
    "<b>Challenge Aim: </b><p align=\"justify\"> <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56364d-c808-4878-8519-62094f59ce61",
   "metadata": {},
   "source": [
    "<p align=\"justify\">\n",
    "Participants will have to build a machine learning model, which can identify and detect the following objects in a satellite image of a cyclone impacted area: \n",
    "<li>Undamaged Residential Building</li>\n",
    "<li>Damaged Residential Building</li>\n",
    "<li>Undamaged Commercial Building</li>\n",
    "<li>Damaged Commercial Building</li>\n",
    "</p>\n",
    "\n",
    "<div align=\"justify\">In this notebook, we will demonstrate a basic model workflow that can serve as a starting point for the challenge. The basic model has been built to identify and detect \"damaged\" and \"un-damaged\" residential as well as commercial buildings. In this demonstration, we have used only the post event images from the Maxar GeoEye-1, and trained a object detection model using <a href = https://docs.ultralytics.com/>YOLOv8.</a> <div>\n",
    "    \n",
    "<div align=\"justify\"> Please note that this notebook is just a starting point. We have made many assumptions in this notebook that you may think are not best for solving the challenge effectively. You are encouraged to modify these functions, rewrite them, or try an entirely new approach.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521e31a-1dda-4e39-b9cc-82e9e57fa87f",
   "metadata": {},
   "source": [
    "## Load In Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd26cd5-107f-4ccb-b73d-868299174643",
   "metadata": {},
   "source": [
    "To run this demonstration notebook, you will need to have the following packages imported below installed. This may take some time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef370ce6-81bf-47ef-8308-76a1900dbcf3",
   "metadata": {},
   "source": [
    "Run the below cell to install the packages. Note that this cell needs to be run only once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b978a6ef-84e3-4840-a4d9-f4b1eace77a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.1.18-py3-none-any.whl (716 kB)\n",
      "Collecting py-cpuinfo\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (1.9.1)\n",
      "Collecting thop>=0.1.1\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (3.5.3)\n",
      "Collecting opencv-python>=4.6.0\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (0.13.0a0+8069656)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (4.64.1)\n",
      "Requirement already satisfied: psutil in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (5.9.2)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (9.2.0)\n",
      "Requirement already satisfied: torch>=1.8.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (1.12.1.post200)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (2.28.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from ultralytics) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (1.23.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (4.37.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from pandas>=1.1.4->ultralytics) (2022.2.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from requests>=2.23.0->ultralytics) (1.26.11)\n",
      "Requirement already satisfied: typing_extensions in /srv/conda/envs/notebook/lib/python3.9/site-packages (from torch>=1.8.0->ultralytics) (4.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Installing collected packages: py-cpuinfo, opencv-python, thop, ultralytics\n",
      "Successfully installed opencv-python-4.9.0.80 py-cpuinfo-9.0.0 thop-0.1.1.post2209072238 ultralytics-8.1.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from opencv-python-headless) (1.23.3)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.9.0.80\n",
      "Collecting labelme2yolo\n",
      "  Using cached labelme2yolo-0.1.4-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from labelme2yolo) (4.9.0.80)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.23.1 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from labelme2yolo) (1.23.3)\n",
      "Requirement already satisfied: pillow<10.3,>=9.2 in /srv/conda/envs/notebook/lib/python3.9/site-packages (from labelme2yolo) (9.2.0)\n",
      "Installing collected packages: labelme2yolo\n",
      "Successfully installed labelme2yolo-0.1.4\n"
     ]
    }
   ],
   "source": [
    "### Run this cell only once.\n",
    "%pip install ultralytics\n",
    "!pip install opencv-python-headless\n",
    "!pip install labelme2yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a35c1c7c-30d1-4672-ac4b-ac7dfc5ce463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GeoTiff Images\n",
    "import rasterio\n",
    "from osgeo import gdal\n",
    "\n",
    "# Visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.pyplot import figure\n",
    "from PIL import Image\n",
    "\n",
    "# Model Building\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import labelme2yolo\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5e025a-9920-4a84-b299-e44e3d42dc89",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99c47d-1938-4762-b537-e68aeb15e48e",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Before building the model, we need to load both Pre-Event and Post Event GeoTiFF Images. We have curated for you data from a certain region in San Juan, Puerto Rico for the year 2017. The variables <b><i>pre_event_image</b></i> and <b><i>post_event_image</b></i> will have the path to both of these images. Consider that the images are considerably large and might require some time to load. The image named Pre_Event_San_Juan.tif is pre event image and is sized at 969 MB, while the image named as Post_Event_San_Juan.tif is post event image and sized at 1.2 GB.</div><br>\n",
    "\n",
    "<div align =\"justify\">To download the images please visit the <a href = \"https://challenge.ey.com/challenges/tropical-cyclone-damage-assessment-lnucmbo5\"><b>data description</b></a> section of the challenge portal or participants can use the wget command to download the files in the background. Wget is the non-interactive network downloader which is used to download files from the server even when the user has not logged on to the system and it can work in the background without hindering the current process. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e88b74e-02a7-4cf8-9be8-f90ea6e49150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-23 15:09:25--  https://challenge.ey.com/api/v1/storage/admin-files/Pre_Event_San_Juan.tif\n",
      "Resolving challenge.ey.com (challenge.ey.com)... 52.236.158.32\n",
      "Connecting to challenge.ey.com (challenge.ey.com)|52.236.158.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1016993213 (970M) [application/octet-stream]\n",
      "Saving to: ‘Pre_Event_San_Juan.tif’\n",
      "\n",
      "Pre_Event_San_Juan. 100%[===================>] 969.88M  78.8MB/s    in 14s     \n",
      "\n",
      "2024-02-23 15:09:52 (69.2 MB/s) - ‘Pre_Event_San_Juan.tif’ saved [1016993213/1016993213]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://challenge.ey.com/api/v1/storage/admin-files/Pre_Event_San_Juan.tif -O Pre_Event_San_Juan.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3e2b6f-14ba-470e-8f50-3124b8ecb492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-23 15:10:01--  https://challenge.ey.com/api/v1/storage/admin-files/Post_Event_San_Juan.tif\n",
      "Resolving challenge.ey.com (challenge.ey.com)... 52.236.158.32\n",
      "Connecting to challenge.ey.com (challenge.ey.com)|52.236.158.32|:443... connected.\n",
      "HTTP request sent, awaiting response... ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://challenge.ey.com/api/v1/storage/admin-files/Post_Event_San_Juan.tif -O Post_Event_San_Juan.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62b6804-5ef2-4e37-93e7-e739a7235322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_event_image = './Pre_Event_San_Juan.tif'\n",
    "post_event_image ='./Post_Event_San_Juan.tif'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d074b-b1be-4c6a-9556-6833cde3140e",
   "metadata": {},
   "source": [
    "#### What are GeoTIFF Images ?\n",
    "\n",
    "<div align = \"justify\">The GeoTIFF format is a metadata standard openly available to the public, which permits the incorporation of georeferencing data into a TIFF file. Extra possible information like map projection, coordinate systems, ellipsoids, datums, and all other required elements for setting the specific spatial reference for the file can be included. The GeoTIFF format is completely aligned with TIFF 6.0, hence, even if a software is unable to read and understand the distinctive metadata, it can still operate a GeoTIFF format file.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138e1d4-26b8-4947-8bf7-b543f751805d",
   "metadata": {},
   "source": [
    "#### Maxar GeoEye-1 Panchromatic Images\n",
    "\n",
    "<div align = \"justify\">Maxar’s visual panchromatic band is a unique high-resolution product that applies RGB (red, green, blue) spectral information to a grey-scale panchromatic image at 30-cm resolution. No cloud masking has been provided with the dataset, so users should be cautious with the data as there are clouds in each of the images. A careful review of the RGB bands (see below) shows the clouds as saturated (dark) regions in each color band. You will find that the pre-storm image has far more clouds (mostly lower left) than the post-storm image (mostly upper portion).</div><br>\n",
    "\n",
    "<div align = \"justify\">If you face kernel crash issue, while visualizing the images on notebook, you can visualize the images on <a href = \"https://qgis.org/en/site/\"><b>QGIS</b></a>, which is a free and open source Geographic Information System.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6097aca-f6b7-4cb8-bed4-06a0a95e8662",
   "metadata": {},
   "source": [
    "## Visualising GeoTiFF Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0054b2e3-e74c-4df3-9f57-2b5e90a7c5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the geotiff file\n",
    "def load_and_visualize(image_path):\n",
    "    with rasterio.open(image_path) as src:\n",
    "        # read the red, green and blue bands\n",
    "        red = src.read(1)\n",
    "        green = src.read(2)\n",
    "        blue = src.read(3)\n",
    "    # Plot the bands\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    ax1.imshow(red,cmap='Reds')\n",
    "    ax1.set_title('Red Band')\n",
    "    ax2.imshow(green, cmap='Greens')\n",
    "    ax2.set_title('Green Band')\n",
    "    ax3.imshow(blue, cmap='Blues')\n",
    "    ax3.set_title('Blue Band')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db9a7c1e-0f15-4f4a-899f-69e860e4799e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load_and_visualize(pre_event_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58fc58ed-9c3d-4a64-9f37-e766417e47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_and_visualize(post_event_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c86b2-8039-405a-bec7-f0adafb5fbc9",
   "metadata": {},
   "source": [
    "## Data Pre Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15495632-a605-4278-9301-164fc959e55b",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Now that our data is loaded , we can we see that the area is quite huge, processing this is a very difficult task. Since in this demonstration, we are using only post-event image to create training dataset, let us proceed to create grids for the post-event images, this facilitates the processing of GeoTiFF image, and these grids will be saved in a specified directory. We create these grids using a function named <i><b>generate_tiles</b></i>. This function takes a .tiff image and the grid size, then transforms them into grids. In this specific case, we've made grids measuring 512x512. But, you can explore the options of creating smaller and/or larger grid as well. Smaller and larger grids have its own pros and cons. You should do a tradeoff between grid size and labelling effort.</div>\n",
    "\n",
    "<p><b>Note:</b> This process may take few minutes to generate the grids.</p>\n",
    "<b style=\"color:rgb(255, 175, 36)\">revisar esto, aquí dice que solo se usan las imagenes post evento, además se usa un tamaño fijo y para mejorar el entrenamiento faltan poner las imagenes pre evento y ver otras combinaciones de tamaño</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4029d-6f71-441f-9bc9-dfc23b216eaa",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 1</strong></h4>\n",
    "<div align=\"justify\">In this case, we're working under the assumption that images taken after the event will also contain infrastructure that hasn't been damaged. To amplify the number of undamaged infrastructure instances, participants may include pre event images as well to increase the training data size.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5be0d8-3f39-48ea-858b-1bee355e2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tiles(input_file, output_dir,grid_x,grid_y):\n",
    "    ds = gdal.Open(input_file)\n",
    "\n",
    "    # Get image size and number of bands numero de pixeles\n",
    "    width = ds.RasterXSize\n",
    "    height = ds.RasterYSize\n",
    "    num_bands = ds.RasterCount\n",
    "\n",
    "    # Calculate number of tiles in each dimension\n",
    "    num_tiles_x = (width // grid_x)\n",
    "    num_tiles_y = (height // grid_y)\n",
    "\n",
    "    print(f\"Total number of tiles: {num_tiles_x * num_tiles_y}\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each tile and save as a separate TIFF image\n",
    "    for i in range(num_tiles_x):\n",
    "        for j in range(num_tiles_y):\n",
    "            x_offset = i *  grid_x\n",
    "            y_offset = j *  grid_y\n",
    "\n",
    "            tile_width = min(grid_x, width - x_offset)\n",
    "            tile_height = min(grid_y, height - y_offset)\n",
    "\n",
    "            tile = []\n",
    "            for band in range(1, num_bands + 1):\n",
    "                tile_data = ds.GetRasterBand(band).ReadAsArray(x_offset, y_offset, tile_width, tile_height)\n",
    "                tile.append(tile_data)\n",
    "\n",
    "            # Create output filename\n",
    "            output_file = os.path.join(output_dir, f\"tile_{i}_{j}.tif\")\n",
    "                \n",
    "            # Create an output TIFF file with same CRS and band values range\n",
    "            driver = gdal.GetDriverByName(\"GTiff\")\n",
    "            options = ['COMPRESS=DEFLATE', 'PREDICTOR=2', 'TILED=YES']\n",
    "            out_ds = driver.Create(output_file, tile_width, tile_height, num_bands, \n",
    "                       ds.GetRasterBand(1).DataType, options=options)\n",
    "            # out_ds = driver.Create(output_file, tile_width, tile_height, num_bands, ds.GetRasterBand(1).DataType)\n",
    "\n",
    "            # Set the geotransform\n",
    "            geotransform = list(ds.GetGeoTransform())\n",
    "            geotransform[0] = geotransform[0] + x_offset * geotransform[1]\n",
    "            geotransform[3] = geotransform[3] + y_offset * geotransform[5]\n",
    "            out_ds.SetGeoTransform(tuple(geotransform))\n",
    "\n",
    "            # Set the projection\n",
    "            out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "            # Write each band to the output file\n",
    "            for band in range(1, num_bands + 1):\n",
    "                out_band = out_ds.GetRasterBand(band)\n",
    "                out_band.WriteArray(tile[band - 1])\n",
    "\n",
    "            # Close the output file\n",
    "            out_ds = None\n",
    "\n",
    "    print(\"Tiles generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff7546-6c3a-481c-817d-551f73ce2570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tiles: 10730\n"
     ]
    }
   ],
   "source": [
    "input_file = \"./Pre_Event_San_Juan.tif\"\n",
    "output_dir = \"./Pre_Event_Grids_In_TIFF\"\n",
    "grid_x = 512\n",
    "grid_y = 512\n",
    "generate_tiles(input_file, output_dir,grid_x,grid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea27ff5-d4ea-4027-aafc-f85e9fe57f61",
   "metadata": {},
   "source": [
    "## Annotating the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96acd709-2ae7-4932-8d12-c2c27dfafa0a",
   "metadata": {},
   "source": [
    "<div align =\"justify\">Image annotation is the process of labelling images in a given dataset to train machine learning models. When the manual annotation is completed, labelled images are processed by a machine learning or deep learning model to replicate the annotations without human supervision. Image annotation sets the standards, which the model tries to copy, so any error in the labels is replicated too. Therefore, precise image annotation lays the foundation for neural networks to be trained, making annotation one of the most important tasks in computer vision.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8763fb-b836-4c34-92a7-feaa92117842",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Inorder to annotate images, we will convert them to .jpg images as majority of the annotaiton tools do not support .tif format. There are many open source annotation tools available. Participants can choose any of the annotation tools. Here a list of few annotation tools that participants can use:</div>\n",
    "<ul>\n",
    "    <li>LabelMe</li>\n",
    "    <li>VGG Image Annotator</li>\n",
    "    <li>LabelImg</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7e3ba-6085-4d5e-847b-54dc60e43eaf",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 2</strong></h4>\n",
    "<div align=\"justify\">Each annotation instrument provides a unique image annotation file format. Choose the annotation tool depending on the model you plan to construct. Also, note that numerous supplementary scripts exist that assist in converting one kind of image annotation file into another.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67928b4b-d58b-4961-83d4-bf2cdc85f494",
   "metadata": {},
   "source": [
    "<div align='justify'>In this demonstration notebook, we will be using <a href = \"https://pypi.org/project/labelme/\">LabelMe</a> to annotate the images. Since LabelMe does not support .Tiff format we will convert these grids to .jpg/.png format.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec94b72e-a3a5-4424-a491-4bc1d04cd2a5",
   "metadata": {},
   "source": [
    "### Identifying Damaged and Undamged Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89399a71-40ca-4fa6-82f5-e912b48854cf",
   "metadata": {},
   "source": [
    "<div align= \"justify\">Commercial structures often have larger rooftops and typically have parking areas. In contrast, residential buildings generally have more limited roof space. Buildings deemed as damaged often exhibit roof impairments, while those classified as undamaged have no visible damages. To further grasp the distinctions between damaged and undamaged buildings in both commercial and residential categories, please refer the <a href = \"https://challenge.ey.com/api/v1/storage/admin-files/2513955341204317-65bb9169868dc8fadbfc9728-2024%20EY%20Open%20Science%20Data%20Challenge%20Participant%20Guidance.pdf\">guidance document</a>. The document is designed to provide a more detailed and comprehensive understanding of these topics.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd604d1-908f-44b4-9664-76c34c1be913",
   "metadata": {},
   "source": [
    "<div align = \"justify\">In conclusion, it is safe to assert that the most challenging aspect of the entire model-building task is discerning between residential and commercial structures and the pinpointing of damaged building. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb9003-9d69-4583-827b-86ad9d61f914",
   "metadata": {},
   "source": [
    "### Converting Images to .png/.jpg images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e0019-18b3-47be-aab0-bb77251b4678",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Since our annotation tool requires images to be in .jpg/.png format, let's proceed to convert the grids from .tiff format to .jpg format, this facilitates the processing of GeoTiFF image, and these grids in .jpg/.png will be saved in a specified directory. We convert these grids using a function named <i><b>convert_tiff_to_jpeg</b></i>.This function takes the folder with .tiff image as input and the output directory path, where these converted images need to be saved.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c7fa0-91fa-4982-9a4f-d145a56df2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_tiff_to_jpeg(input_dir,output_dir):\n",
    "    # check if output_dir exists, if not create it\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        # check if file is an image (ends with .tif)\n",
    "        if filename.endswith('.tif'):\n",
    "            img = Image.open(os.path.join(input_dir, filename))\n",
    "        \n",
    "            # check if image is RGB mode, if not convert it\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "        \n",
    "            # create new filename, replace .tif with .jpg\n",
    "            output_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        \n",
    "            # save the image in JPEG format\n",
    "            img.save(os.path.join(output_dir, output_filename), 'JPEG')\n",
    "    print(\"Conversion from TIFF to JPEG completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc908c-5ac1-4544-bc63-9dfbdc7e6da1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify directory\n",
    "input_dir = \"./Pre_Event_Grids_In_TIFF\"\n",
    "output_dir = \"./Pre_Event_Grids_In_JPEG\"\n",
    "convert_tiff_to_jpeg(input_dir,output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3e64a-3fcd-4f4c-8ee4-7759f2756e22",
   "metadata": {},
   "source": [
    "### Renaming the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75463ed9-0800-42c9-af4a-3e2babd916b6",
   "metadata": {},
   "source": [
    "<div align = \"justify\">For easier and more efficient data accessibility, it's necessary to rename the files in the directory. We'll use the function <b><i>rename_files</b></i> to accomplish this task of altering the file names in the given path.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75595d54-1bba-4e8f-b8b3-3d7eec05b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(directory_path):\n",
    "# Define the directory path where your files are located\n",
    "    directory_path = directory_path\n",
    "    \n",
    "    # Get a list of all files in the directory\n",
    "    files = os.listdir(directory_path)\n",
    "    \n",
    "    # Define a prefix for the new file names \n",
    "    # Change the prefix as per requirement\n",
    "    prefix = \"Post_Event_\"\n",
    "    \n",
    "    # Start the numbering from 1\n",
    "    number = 0\n",
    "    \n",
    "    # Loop through each file in the directory\n",
    "    for filename in files:\n",
    "        # Check if the item is a file (not a directory)\n",
    "        if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "            # Get the file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "    \n",
    "            # Create the new file name with leading zeros\n",
    "            new_filename = f\"{prefix}{number:03}{file_extension}\"\n",
    "    \n",
    "            # Construct the full path to the original and new files\n",
    "            old_filepath = os.path.join(directory_path, filename)\n",
    "            new_filepath = os.path.join(directory_path, new_filename)\n",
    "    \n",
    "            # Rename the file\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "    \n",
    "            # Increment the number for the next file\n",
    "            number += 1\n",
    "    \n",
    "    print(\"Files renamed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ce461-6a16-4585-bed2-6733ec3aebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f77ac-0d24-42c4-8379-06553588e52b",
   "metadata": {},
   "source": [
    "### LabelMe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bdf2a-fed0-412c-8f9d-18bba95962c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align = \"justify\">LabelMe is a renowned, open-source, graphical annotation tool perfect for annotating images and videos. The tool is Python-based and integrates Qt for its graphical interface. With its light nature and simple to use interface, LabelMe is a preferred option for an open-source visual annotation tool.</div><br>\n",
    "\n",
    "<div align=\"justify\">LabelMe provides functionalities to facilitate annotations for object identification, semantic segmentation, and panoptic segmentation for both image and video data. Additionally, it also aids in addressing various computer vision issues like classification and segmentation. With LabelMe, users can create annotations using circles, rectangles (bounding boxes), lines, and polygons.</div><br>\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"justify\">Interested individuals can find the tool on GitHub, and a comprehensive, step-by-step guide for installing LabelMe can be found <a href=\"https://github.com/wkentaro/labelme\">here</a>. If installation seems daunting or impossible due to system restrictions, LabelMe also provides standalone executive applications for Windows, MacOs, and Linux. Simply access the application <a href=\"https://github.com/wkentaro/labelme/releases/tag/v5.0.2\">here</a> and run the executive file to get started.</div><br>\n",
    "\n",
    "<div align=\"justify\">\n",
    "<h4>Drawing Annotations</h4> <ul> <li>Launch LabelMe and open the directory containing the images you aim to annotate. Opening a directory enables batch processing, which hastens the annotation process.</li> <li>Choose the image you wish to annotate from the file list located at the bottom right corner.</li> <li>Begin the annotation by choosing the 'Create Rectangle' option from the Edit menu. Click to set the rectangle starting point on the image and click on the next key point to form a rectangle. Continue this process until you've completed annotating the image, and click the starting keypoint to conclude the annotation.</li> <li>For different annotation formats, select 'Edit' from the title bar and choose the format you prefer.</li> <li>Indicate the annotation by typing in the label or selecting an option from the label list.</li> <li>By default LabelMe saves your annotation as a JSON file in the same folder as the images.For convenience, you can use the automatically-generated filename. For continuous save, select 'Save Automatically' in the File menu at the top left corner.</li> </ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7c7e82-4167-45de-abb5-0ca5984ce2bc",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 3</strong></h4>\n",
    "<div align=\"justify\">Annotations are integral to the success of object recognition tasks. By labeling or annotating objects in a given set of images, you pinpoint to the model what to look for. Therefore, it is not only advisable but highly beneficial to annotate as many objects as possible.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161a44d",
   "metadata": {},
   "source": [
    "#### Naming convention of class label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b3178",
   "metadata": {},
   "source": [
    "<div align=\"justify\"> Please ensure that you follow the following naming convention for the different objects, while labelling. If a different naming convention is followed, it might lead to error during submission on platform. </div>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Object</th>\n",
    "    <th>Naming Convention</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Undamaged Residential Building</td>\n",
    "    <td>undamagedresidentialbuilding</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Damaged Residential Building</td>\n",
    "    <td>damagedresidentialbuilding</td>\n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td>Undamaged Commercial Building</td>\n",
    "    <td>undamagedcommercialbuilding</td>\n",
    "  </tr>\n",
    "     <tr>\n",
    "    <td>Damaged Commercial Building</td>\n",
    "    <td>damagedcommercialbuilding</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b31133-c396-41b3-a4cd-100d5d5a004d",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align = \"justify\">After the annotation process, participants can move onto the task of building their models. YOLOv8, however, requires annotations to be in a distinct format. Therefore, the next step is to convert these annotations into the YOLOv8-required format. There are numerous utility scripts/packages designed to facilitate the conversion from .json to .txt files, which YOLOv8 requires. In this case, we have utilized the labelme2yolo package to transform the .json file into .txt format and also to generate the configuration file (dataset.yaml)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2190d59-17ad-4810-98b6-77058290bbb5",
   "metadata": {},
   "source": [
    "Here we have annotated around 100 images of which 80 images will be used for training and the rest 20 as test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b4a1cd-5190-41a6-96e6-c649ffdb2961",
   "metadata": {},
   "source": [
    "### YOLO Annotations Format and Train Test Split\n",
    "\n",
    "<div align=\"justify\">Since, YOLO algorithms requires the annotations to be in a specific format labels generated using LabelMe annotation tool should be exported to YOLO format with one *.txt file per image. If there are no objects in an image, no *.txt file is required. In case of bounding box annotations the *.txt file should be formatted with one row per object in class x_center y_center width height format. Box coordinates must be in normalized xywh format (from 0 to 1). If your boxes are in pixels, you should divide x_center and width by image width, and y_center and height by image height. Class numbers should be zero-indexed (start with 0).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa5eee-9167-418d-8d7e-26e76954a71c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div align = \"justify\">We have annotated 100 images using LabelMe and have converted them to .txt format using the <a href = https://pypi.org/project/labelme2yolo/>LabelMe2Yolo</a> package. We utilized the package to divide the data into training and testing sets, and also to create a .yaml file. By default, labelme2yolo package converts LabelMe annotations to <a href =\"https://docs.ultralytics.com/datasets/obb/#yolo-obb-format\"> YOLO OBB </a> format which has been used to build this model. The YOLO OBB format designates bounding boxes by their four corner points with coordinates normalized between 0 and 1. It follows this format: class_index, x1, y1, x2, y2, x3, y3, x4, y4. Internally, YOLO processes losses and outputs in the xywhr format, which represents the bounding box's center point (xy), width, height, and rotation. In order to get annotations in bounding box format  (x,y,w,h) you need to pass the parameter <i>--output_format bbox</i>. Participants could consider the approach of using standard bounding boxes, without any specific orientation, as a technique for improving their model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4398b-f23b-408f-ab02-1d809c431329",
   "metadata": {},
   "source": [
    "<div align = \"justify\">For more details on how to config the train and test validation split, please refer <a href = https://pypi.org/project/labelme2yolo/>LabelMe2Yolo</a>.By default 80% of the data is considered for Training Data and the rest of the 20% for the test data. Participants can modify this by changing the parameters.</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9454e-4f30-497d-a6f8-c5ca642c51b6",
   "metadata": {},
   "source": [
    "#### Generating Train and Test Data along with Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d151b-277c-46cc-8765-fb6861297af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !labelme2yolo --json_dir /path/to/labelme_json_dir/\n",
    "!labelme2yolo --json_dir ./Annotated_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d553ae8-1804-48a5-b573-dffe1ab7a82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def move_files(source, dest):\n",
    "    os.rename(source, dest)\n",
    "    print(\"File moved successfully.\")\n",
    "\n",
    "os.mkdir(\"./datasets/\")\n",
    "os.mkdir(\"./datasets/train/\")\n",
    "os.mkdir(\"./datasets/val/\")\n",
    "\n",
    "move_files(\"./Annotated_Data/YOLODataset/dataset.yaml\",\"./dataset.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a63bc-2ec3-4269-bc44-a5085ea4b797",
   "metadata": {},
   "source": [
    "## Preparing the File Structure as required by YOLO \n",
    "<div align =\"justify\">The participants are required to construct the file structure as indicated below and adjust the dataset.yaml settings according to the file path provided.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e61604-b18b-4506-8718-445be3bd618a",
   "metadata": {},
   "source": [
    "### File Structure\n",
    "<div align=\"justify\">YOLOv8, being a Deep Learning model for object detection, necessitates a specific directory framework for running effectively. The fundamental structure has a parent folder titled 'datasets', which further contains two subfolders: 'train', and 'val'.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'datasets' folder: As the name indicates, this folder houses the complete data that YOLOv8 will be working on. It is pivotal as the model gains insight, verifies accuracy, and enhances its effectiveness using the data stored here.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'train' subfolder: This is a subset of the datasets folder and houses a collection of both image files (.jpg format) and their corresponding annotation files (.txt format). The training data is used by the model to learn and create a mathematical function that can predict the output.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'val' subfolder: Similarly, this contains the test images and their corresponding annotations. The test data is not learned by the model. Instead, it's used to evaluate the performance of the model's learning.</div><br>\n",
    "\n",
    "<div align=\"justify\">The 'dataset.yaml' file: This is a vital document that encompasses critical details concerning the dataset and the manners of their use. It contains paths for training, testing, and their respective numbers of images, the total count of classes, and class labels. This is generated in the previous section - <b>YOLO Annotations Format and Train Test Split</b></div><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb07eb-b917-479a-90e0-7b93cb2707a5",
   "metadata": {},
   "source": [
    "The files need to be organised in the following manner:\n",
    "<ul>\n",
    "    <li> dataset.yaml </li>\n",
    "    <li>datasets</li>\n",
    "<ul>\n",
    "    <li> train </li>\n",
    "    <ul>\n",
    "        <li>image1.jpg</li>\n",
    "        <li>image1.txt</li>\n",
    "        <li>image2.jpg</li>\n",
    "        <li>image2.txt</li>\n",
    "        <li>image3.jpg</li>\n",
    "        <li>image3.txt</li>\n",
    "    </ul>\n",
    "    <li> val</li>\n",
    "    <ul>\n",
    "        <li>image4.jpg</li>\n",
    "        <li>image4.txt</li>\n",
    "        <li>image5.jpg</li>\n",
    "        <li>image5.txt</li>\n",
    "        <li>image6.jpg</li>\n",
    "        <li>image6.txt</li>\n",
    "    </ul>\n",
    " \n",
    "</ul>\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123682e-8ef8-45da-8130-9450b6601efe",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "<div align=\"justify\">\n",
    "\n",
    "Once the file structure is set up, we can proceed to the exercise of building the model. This process is iterative in nature, as participants have to keep experimenting and tweaking our model’s settings to improve its performance. Throughout this point, we continually assess the performance of our model using metric like mean average precission (mAP).</div>\n",
    "\n",
    "<div align=\"justify\">Post-training, the model needs to be validated using a different dataset to further confirm that it works accurately when exposed to unseen data. Lastly, our model should also ensure it is not overfit or underfit. An overfit model implies it is too complex and may not work well with new data, while an underfit model implies the model misses the relevant relations between features and target outputs.</div><br>\n",
    "\n",
    "<div align=\"justify\">Once we complete these steps, we will have a working model that we can now use for various predictions and interpretations.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bde60-6841-4742-b94a-834e3bfc800e",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4970ce6-a6ed-47f3-9c6b-f5d19d520030",
   "metadata": {},
   "source": [
    "<div align =\"justify\">Now that we have the data in a format appropriate for developing the model, we can begin training a model. In this demonstration notebook, we have used pre-trained object detection model from YOLOv8 developed by Ultralytics. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customization capabilities.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae54052-eefe-46b5-a1ed-d446a0b1e46f",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 4</strong></h4>\n",
    "<div align =\"justify\">Ultralytics offers different object detection models which vary in size and the number of parameters it learns from the images. Participants can try the various models offered by Ultralytics to improve the accuracy of detection. For details regrading the different object detection models offered by Ultralytics please refer <a href = \"https://docs.ultralytics.com/models/yolov8/\">here.</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ff816-d963-4891-902a-7829f5e656dd",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 5</strong></h4>\n",
    "<div align=\"justify\">Here, we are using YOLOv8 (yolov8n.pt) to build a custom object detection model. Participants can explore other object detection models or come up with a totally different approach inorder to identify the damaged and undamaged buildings. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab008c6-ef91-46af-89a0-9d6b7f4bda17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "model = YOLO('yolov8n.pt')\n",
    "# Display model information (optional)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a3a70-bc5e-4c83-91ee-440ffe4b90bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the dataset for 50 epochs\n",
    "results = model.train(data='./dataset.yaml', epochs=50, imgsz=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca787c9f-e2de-4e6e-bb34-10f46390de82",
   "metadata": {},
   "source": [
    "<b>Note: Participants, can find the model weights in runs\\detect\\ ....</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb95b28-1298-42cb-a2a2-64ca4795b385",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "<div align =\"justify\">Now that we have trained our model , all that is left is to evaluate it. For evaluation we will generate the mean average precission report. We will then plot the image results to visualise the results. When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalize. This is because models have a tendency to overfit the dataset they are trained on. To estimate the in-sample and out-of-sample performance, we will see the results using the graph now.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66c90e-4121-4b4c-a30f-34b9f7a1669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(15, 10), dpi=80)\n",
    "# reading the image \n",
    "results = img.imread('runs/detect/train2/results.png')   \n",
    "# displaying the image \n",
    "plt.imshow(results) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187ded6-a4d6-49d8-8f2a-d1e5adb497be",
   "metadata": {},
   "source": [
    "<div align =\"justify\">From the above results we can see that we achieved an overall MAP-50 of 0.34 and the following MAP-50 on the various classes:\n",
    "<ul>\n",
    "    <li>Undamaged Commercial Building -0.29</li>\n",
    "    <li>Undamaged Residential Building -0.69</li>\n",
    "    <li>Damaged Residential Building - 0.23</li>\n",
    "    <li>Damaged Commercial Building -0.16</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622efaaf-3ca9-4f7a-8a82-77922b883ff4",
   "metadata": {},
   "source": [
    "Let us now look at the confusion matrix, to better analyse the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3545df-adf7-4d6c-83ac-e1d8152e073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(20,15), dpi=80)  \n",
    "# reading the image \n",
    "cf = img.imread('runs/detect/train2/confusion_matrix.png') \n",
    "# displaying the image \n",
    "plt.imshow(cf) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32649f-2d09-43e1-91dd-1f6466618b8d",
   "metadata": {},
   "source": [
    "<div align =\"justify\">From the above, we see that the model is able to achieve overall mAP score of <b>0.34</b>. This is not a very good score, so your goal is to improve this score. Despite its decent performance in identifying undamaged residential buildings, with an mAP score of 0.69, the model struggles when it comes to detecting commercial buildings. Furthermore, the results highlight the inherent challenge in identifying damage in both residential and commercial structures.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec8647c-4338-454b-9b6c-2d06bf0bbcce",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 6</strong></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b469eaf-444b-465a-af1a-289edce39690",
   "metadata": {},
   "source": [
    "<div align = \"justify\">Participants have the opportunity to improve their model's performance by focusing on classes where the model's object detection is failing, and providing more labels for instances of that class. This could potentially enhance the mean average precision (mAP) score. Furthermore, creating synthetic data is another strategy participants could adopt to boost their results. This artificial data creation has the potential to enrich their model's learning experience and subsequently escalate its predictive performance. In addition, participants are encouraged to explore various other generative AI image models that have the ability to generate synthetic images. This strategy can enhance the training data quantity, which may subsequently result in improved outcomes.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a9f7c-37e7-410b-8884-9108c38b9e4d",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c08438-7517-4b43-baf0-0f9077bd454f",
   "metadata": {},
   "source": [
    "<div align = \"justify\">Once you are happy with your model, you can make a submission. To make a submission, you will need to use your model to make predictions on the images we have provided in the <b>\"Data Description (Submission)\"</b> section and upload the file onto the challenge platform.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb347ca-25e2-43b2-9ddf-a73a34eb5b6d",
   "metadata": {},
   "source": [
    "<div align=\"justify\">Please ensure you do not alter the validation file names (.jpg format), prediction file names (.txt format), as well as do not insert spaces within class names in prediction files. If this is done, the submission process will not generate score on scorecard in platform.</div><br>\n",
    "\n",
    "Here is the format of the prediction of classes, which should be followed in prediction files (.txt format).<br>\n",
    "&lt;class_name&gt; &lt;confidence&gt; &lt;left&gt; &lt;top&gt; &lt;right&gt; &lt;bottom&gt;\n",
    "\n",
    "For example:\n",
    "\n",
    "undamagedresidentialbuilding 0.8850483894348145 91.77708435058594 9.232025146484375 135.427978515625 61.545249938964844\n",
    "\n",
    "damagedresidentialbuilding 0.39302119612693787 417.7862243652344 72.88447570800781 490.5438537597656 124.75260925292969"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ebdc5",
   "metadata": {},
   "source": [
    "### Download submission images from platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aa9e4a-94df-4069-b548-26bb33a51764",
   "metadata": {},
   "source": [
    "Participants need to download the images from the submission section of the challenge portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da85fa-07b8-4b77-a469-7edc0a072008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_folder(zip_filepath, dest_dir):\n",
    "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_dir)\n",
    "    print(f'The zip file {zip_filepath} has been extracted to the directory {dest_dir}')\n",
    "\n",
    "\n",
    "\n",
    "submission_zip = './challenge_1_submission_images.zip'\n",
    "submission_directory = './challenge_1_submission_images'\n",
    "unzip_folder(submission_zip,submission_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202b019-a9b0-43b4-807e-da0d796125d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Making Predictions on the Submission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89289c8c-0f6e-4474-8664-a2b76fd46819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "model = YOLO('./best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308cf17b-5ce8-4f81-b592-de944d0db8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding according to the .yaml file class names order\n",
    "decoding_of_predictions ={0: 'undamagedcommercialbuilding', 1: 'undamagedresidentialbuilding', 2: 'damagedresidentialbuilding', 3: 'damagedcommercialbuilding'}\n",
    "\n",
    "directory = 'challenge_1_submission_images/Validation_Data_JPEG'\n",
    "# Directory to store outputs\n",
    "results_directory = 'Validation_Data_Results'\n",
    "\n",
    "# Create submission directory if it doesn't exist\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the current object is a file and ends with .jpeg\n",
    "    if os.path.isfile(os.path.join(directory, filename)) and filename.lower().endswith('.jpg'):\n",
    "        # Perform operations on the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        print(file_path)\n",
    "        print(\"Making a prediction on \", filename)\n",
    "        results = model.predict(file_path, save=True, iou=0.5, save_txt=True, conf=0.25)\n",
    "        \n",
    "        for r in results:\n",
    "            conf_list = r.boxes.conf.numpy().tolist()\n",
    "            clss_list = r.boxes.cls.numpy().tolist()\n",
    "            original_list = clss_list\n",
    "            updated_list = []\n",
    "            for element in original_list:\n",
    "                 updated_list.append(decoding_of_predictions[int(element)])\n",
    "\n",
    "        bounding_boxes = r.boxes.xyxy.numpy()\n",
    "        confidences = conf_list\n",
    "        class_names = updated_list\n",
    "\n",
    "        # Check if bounding boxes, confidences and class names match\n",
    "        if len(bounding_boxes) != len(confidences) or len(bounding_boxes) != len(class_names):\n",
    "            print(\"Error: Number of bounding boxes, confidences, and class names should be the same.\")\n",
    "            continue\n",
    "        text_file_name = os.path.splitext(filename)[0]\n",
    "        # Creating a new .txt file for each image in the submission_directory\n",
    "        with open(os.path.join(results_directory, f\"{text_file_name}.txt\"), \"w\") as file:\n",
    "            for i in range(len(bounding_boxes)):\n",
    "                # Get coordinates of each bounding box\n",
    "                left, top, right, bottom = bounding_boxes[i]\n",
    "                # Write content to file in desired format\n",
    "                file.write(f\"{class_names[i]} {confidences[i]} {left} {top} {right} {bottom}\\n\")\n",
    "        print(\"Output files generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27f73f-5e92-47dc-b744-1a4abaefb584",
   "metadata": {},
   "source": [
    "<b>Note: The submission file should contain only the .txt files and not the images.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441472f-f196-4bca-9081-e732db9dca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your source directory and the destination where the zip file will be created\n",
    "source_dir = results_directory\n",
    "destination_zip = 'submission'\n",
    "\n",
    "# Create a zip file from the directory\n",
    "shutil.make_archive(destination_zip, 'zip', source_dir)\n",
    "\n",
    "print(f\"Directory {source_dir} has been successfully zipped into {destination_zip}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c3243",
   "metadata": {},
   "source": [
    "### Upload submission file on platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ade1ab",
   "metadata": {},
   "source": [
    "Upload the submission.zip on the <a href =\"https://challenge.ey.com\">platform</a> to get score generated on scoreboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f68d82-b61d-4286-88c7-ec059d4164c1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "<div align =\"justify\">Now that you have learned a basic approach to model training, it’s time to try your own approach! Feel free to modify any of the functions presented in this notebook. We look forward to seeing your version of the model and the results. Best of luck with the challenge!</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
